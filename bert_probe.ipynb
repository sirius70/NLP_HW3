{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirius70/NLP_HW3/blob/main/bert_probe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probling Language Model Representations\n",
        "\n",
        "In this notebook, you will explore how much information language models have about linguistic structure even when they have not been explicitly trained to predict it. You will use the encoder language model BERT.\n",
        "\n",
        "This is a kind of experiment called &ldquo;probing&rdquo;, where we use internal representations from a language model to predict certain information we have but the language model does not. In particular, we will use a named entity recognition (NER) task, `BIO` tags on each word for the classes person, location, organization, and miscellaneous. The base BERT model did not see any of these labels in trainingâ€”although BERT has often been fine-tuned on token labeling tasks. For more on token classification for named entity recognition, and for some of the code we use here, see [this huggingface tutorial](https://huggingface.co/docs/transformers/en/tasks/token_classification).\n",
        "\n",
        "Work through the notebook and complete the cells marked TODO to set up and run these experiments.\n",
        "\n",
        "We start by installing the huggingface `transformers` and related libraries."
      ],
      "metadata": {
        "id": "Oqutyn9tAxNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS4RG2THt6r_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you want them later, we'll load the sklearn functions you used for training logistic regression in assignment 2."
      ],
      "metadata": {
        "id": "dXUDVxhsEsgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate, LeaveOneOut, KFold\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hayfqZB_CQXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll use the huggingface `datasets` library to download the CoNLL (Conference on Natural Language Learning) 2003 data for named-entity recognition."
      ],
      "metadata": {
        "id": "vR0WURzzE6aS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySwBDWgry94w"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "conll2003 = load_dataset(\"hgissbkh/conll2003-en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep things simple, we'll work with a sample of 1000 sentences."
      ],
      "metadata": {
        "id": "RDGXsyMdFKTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDLq5VnHud0y"
      },
      "outputs": [],
      "source": [
        "sample = conll2003['train'].select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each record contains a list of word tokens and a list of NER labels. For efficiency, the labels have been turned into integers, which makes them hard to interpret."
      ],
      "metadata": {
        "id": "RucYFBjMFQkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE8Y4d5cvrJG"
      },
      "outputs": [],
      "source": [
        "sample[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, the dataset object also contains information to map these integers back to readable strings. We can see tags such as `B-PER` (the beginning token of a personal name), `I-PER` (the following tokens inside a personal name, if any), and `O` (a token outside any named entities). We create two dictionaries `id2label` and `label2id` to make mapping between integers and labels easier."
      ],
      "metadata": {
        "id": "yrXioS9_F4XT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4un7cWKOujei"
      },
      "outputs": [],
      "source": [
        "labels = sample.features['ner'].feature.names\n",
        "id2label = {i: label for i, label in enumerate(labels)}\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "print(labels)\n",
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a language model to interpret our data properly, we need to tokenize it in the same way as its training data. We download the tokenizer for the `bert-base-cased` model from huggingface."
      ],
      "metadata": {
        "id": "74oOTKJjG1lu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAM3kiN0Ir_0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens when we run the tokenizer on a single sentence. We tell it that our sentence has already been split into words, in this case by the creators of the CoNLL 2003 NER dataset. BERT, like many language models, used **subword tokenization** to keep the size of its vocabulary manageable. The tokenizer turns $n$ words into $m \\ge n$ tokens, represented as a list of integer token identifiers. We use the method `convert_ids_to_tokens` to turn these integers back into a string representation."
      ],
      "metadata": {
        "id": "ZDLvbjoQPodP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP5X7k98I3D8"
      },
      "outputs": [],
      "source": [
        "example = sample[10]\n",
        "tokenized_input = tokenizer(example['words'], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the name `Palacio` has been split into three subword tokens: `Pa`, `##la`, and `##cio`. The prepended `##` indicates that this token is _not_ the start of a word. But the NER annotations we have are at the word level. We thus need to do some work to map the sequence of NER labels, linked to words, to the usually longer sequence of subword tokens. This is a common task when you have data that wasn't created for a particular language model's classification. We adapt a function from the huggingface tutorial to map the NER labels onto the subword tokens. We assign the label -100 to tokens not at the beginning of a word, as well as to the sentinel `[CLS]` and `[SEP]` tokens at the beginning and end of the sentence."
      ],
      "metadata": {
        "id": "ywXS7MT3Xi7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3k7NQZsJvxj"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples['words'], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['ner']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply this function to the whole dataset."
      ],
      "metadata": {
        "id": "fvZMEdiVZZQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31LYPwDBJ_mf"
      },
      "outputs": [],
      "source": [
        "tokenized_sample = sample.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_sample.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each record in the tokenized sample now has numeric IDs for each token, an attention mask (always 1 in this encoding task), and token-level labels."
      ],
      "metadata": {
        "id": "eNRGHrodeUU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sample[0]"
      ],
      "metadata": {
        "id": "MeQTGdLTeKJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the BERT model itself. We use the version that was trained on data that hadn't been case-folded, since upper-case words might be useful features for NER in English."
      ],
      "metadata": {
        "id": "AA2YL1y5ZpY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4pyKmWmOQGm"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run inference on the first sentence in our sample, passing the model the list of token identifiers (coerced into a tensor with a single batch dimension) and the attention mask, which is all 1s for this simple encoding task."
      ],
      "metadata": {
        "id": "SLKHgAlKbEYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLEefcWKPQsJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids=tokenized_sample[0]['input_ids'].unsqueeze(0), attention_mask=tokenized_sample[0]['attention_mask'].unsqueeze(0))\n",
        "  hidden_states = outputs.hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `hidden_states` object we just created is a tuple with 13 items, one for each layer of the BERT model. The initial token embedding is layer 0 and the output is layer 12. Each layer contains embeddings for each token&mdash;here, there are 12&mdash;each of which is a vector of length 768."
      ],
      "metadata": {
        "id": "EAORN7nWbomy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(hidden_states))\n",
        "print(hidden_states[0].shape)"
      ],
      "metadata": {
        "id": "fByOPUHA6kQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define a function to take a dataset of tokens, run it through BERT to produce embeddings at all 13 layers, and to produce features for predicting NER labels from token embeddings. This function uses two explicit nested loops, which is not the fastest way to do things in pytorch, but more clearly expresses what is being computed. It takes about a minute to run on colab. (This assignment isn't meant to be a pytorch tutorial, but if you know pytorch, or are learning it, feel free to speed up this code by batching the examples together.)"
      ],
      "metadata": {
        "id": "EAqhsDYbf9gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_layer_representation(data, model, tokenizer):\n",
        "  rep = []\n",
        "  lab = []\n",
        "  for example in data:\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids=example['input_ids'].unsqueeze(0), attention_mask=example['attention_mask'].unsqueeze(0))\n",
        "      tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n",
        "      hidden_states = outputs.hidden_states\n",
        "      for i in range(len(example['labels'])):\n",
        "        if example['labels'][i] != -100:\n",
        "          lab.append(int(example['labels'][i]))\n",
        "          rep.append([hidden_states[layer][0][i].numpy() for layer in range(len(hidden_states))])\n",
        "          #rep.append(hidden_states[layer][0][i].numpy())\n",
        "  return [np.array(rep), np.array(lab)]"
      ],
      "metadata": {
        "id": "fALaldOVFKl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute embeddings for all layers for the full dataset. Note that the first dimension is now _words_ rather then sentences. This means that we can probe the information that each word's embedding has about named entities (or anything else)."
      ],
      "metadata": {
        "id": "lEcOfqfk89-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = compute_layer_representation(tokenized_sample, model, tokenizer)"
      ],
      "metadata": {
        "id": "2jgY5wxFezYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can select information about the bottom (word embedding) layer, which gives as a matrix of words by embedding dimensions."
      ],
      "metadata": {
        "id": "3Cyirs1a9WOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[:,0,:].shape"
      ],
      "metadata": {
        "id": "P82cuK3wfLiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Your first task is to probe the information that these emedding layers have about named entities. Train one linear model for each of the 13 layers of BERT to predict the label of each word in `y` using the embeddings in `X`. Print the accuracy of this model for each of the 13 layers of BERT. By accuracy, we simply mean the proportion of words that have been assigned the correct tag. (Although NER is often evaluated at the level of the entity, which may span one or more words, we will keep things simple here.)\n",
        "\n",
        "You may use the sklearn code for training logistic regression models that you ran in assignment 2. You may also train these classifiers using pytorch. In any case, perform 10-fold cross validation and return the average accuracy over all ten folds."
      ],
      "metadata": {
        "id": "4D4n9hyl9xF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train linear models to predict the NER labels using embeddings from each layer of BERT.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# We have:\n",
        "# X: shape (num_tokens, 13, 768)\n",
        "# y: shape (num_tokens,)\n",
        "# We'll loop through 13 layers (0â€“12), take X[:, layer, :], and train a classifier.\n",
        "\n",
        "n_layers = X.shape[1]\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "layer_accuracies = []\n",
        "\n",
        "print(\"Training logistic regression probes for each BERT layer...\\n\")\n",
        "\n",
        "for layer in range(n_layers):\n",
        "    X_layer = X[:, layer, :]  # Extract embeddings for this layer\n",
        "    accuracies = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X_layer):\n",
        "        X_train, X_test = X_layer[train_index], X_layer[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        clf = LogisticRegression(max_iter=200, solver='lbfgs', n_jobs=-1)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    layer_accuracies.append(mean_acc)\n",
        "    print(f\"Layer {layer:2d}: {mean_acc:.4f}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nAverage accuracy per BERT layer:\")\n",
        "for layer, acc in enumerate(layer_accuracies):\n",
        "    print(f\"Layer {layer:2d}: {acc:.4f}\")\n",
        "\n",
        "# Optional: visualize the trend\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(n_layers), layer_accuracies, marker='o')\n",
        "plt.xlabel(\"BERT Layer\")\n",
        "plt.ylabel(\"NER Accuracy\")\n",
        "plt.title(\"Probing NER Knowledge in Each BERT Layer\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mk5o8DbN-EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train linear models to predict the NER labels using embeddings from each layer of BERT.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "layer_accuracies = []\n",
        "\n",
        "for layer in range(X.shape[1]):\n",
        "    X_layer = X[:, layer, :]\n",
        "    accuracies = []\n",
        "    for train_idx, test_idx in kf.split(X_layer):\n",
        "        X_train, X_test = X_layer[train_idx], X_layer[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        clf = LogisticRegression(max_iter=200, solver='lbfgs', n_jobs=-1)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    layer_accuracies.append(mean_acc)\n",
        "    print(f\"Layer {layer}: {mean_acc:.4f}\")\n",
        "\n",
        "plt.plot(range(len(layer_accuracies)), layer_accuracies, marker='o')\n",
        "plt.xlabel(\"BERT Layer\")\n",
        "plt.ylabel(\"Average Accuracy (10-fold CV)\")\n",
        "plt.title(\"NER Probing Accuracy by BERT Layer\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rFO-xoNT5rSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** How good are these accuracy levels? Since the `O` tag is very common, you can do quite well by always predicting `O`. Compute the baseline accuracy, i.e., the accuracy you would get on the sample data if you always predicted `O`."
      ],
      "metadata": {
        "id": "S38RraNheckq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute and print the baseline accuracy of always predicting O.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# The label ID for 'O' (outside any named entity)\n",
        "o_label_id = label2id['O']\n",
        "\n",
        "# Compute how many tokens in y are 'O'\n",
        "o_count = np.sum(y == o_label_id)\n",
        "\n",
        "# Compute baseline accuracy: proportion of tokens that are 'O'\n",
        "baseline_accuracy = o_count / len(y)\n",
        "\n",
        "print(f\"Baseline accuracy (always predicting 'O'): {baseline_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "MdqzlBMugAUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Now try another probing experiment for capitalized words, a simple feature that, in English, is correlated with named entities. For each word in the sample data, create a feature that indicates whether that word's first character is a capital letter. Then train logistic regression models for each layer of BERT to see how well they predict capitalization. Perform 10-fold cross-validation as above. Note any differences you see with the NER probes.\n",
        "\n",
        "In addition, compute the baseline accuracy, i.e., the accuracy of always predicting that a word is not capitalized."
      ],
      "metadata": {
        "id": "gG73Dw8sgFtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train linear models to predict capitalization.\n",
        "# Compute and print the accuracy of these models for each layer of BERT.\n",
        "# Compute and print the baseline accuracy.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "import numpy as np\n",
        "\n",
        "# --- Step 1: Create binary capitalization labels for each token ---\n",
        "# We'll use the original words from the sample to determine if the first character is uppercase.\n",
        "\n",
        "capitalization_labels = []\n",
        "for example in sample:\n",
        "    for word in example['words']:\n",
        "        if word and word[0].isupper():\n",
        "            capitalization_labels.append(1)\n",
        "        else:\n",
        "            capitalization_labels.append(0)\n",
        "\n",
        "capitalization_labels = np.array(capitalization_labels)\n",
        "\n",
        "# Check alignment (should match y length)\n",
        "assert len(capitalization_labels) == len(y), \"Mismatch between labels and tokens!\"\n",
        "\n",
        "# --- Step 2: Train logistic regression models on each BERT layer ---\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cap_layer_accuracies = []\n",
        "\n",
        "print(\"Training logistic regression models to predict capitalization...\\n\")\n",
        "\n",
        "for layer in range(X.shape[1]):\n",
        "    X_layer = X[:, layer, :]\n",
        "    scores = cross_val_score(\n",
        "        LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1, random_state=42),\n",
        "        X_layer,\n",
        "        capitalization_labels,\n",
        "        cv=kf,\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    mean_acc = scores.mean()\n",
        "    cap_layer_accuracies.append(mean_acc)\n",
        "    print(f\"Layer {layer}: Accuracy = {mean_acc:.4f} (std = {scores.std():.4f})\")\n",
        "\n",
        "# --- Step 3: Compute baseline accuracy ---\n",
        "# Most words are not capitalized\n",
        "non_capitalized = np.sum(capitalization_labels == 0)\n",
        "baseline_accuracy = non_capitalized / len(capitalization_labels)\n",
        "print(f\"\\nBaseline accuracy (always predicting 'not capitalized'): {baseline_accuracy:.4f}\")\n",
        "\n",
        "# --- Step 4: Summarize ---\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"Summary of capitalization accuracies across layers:\")\n",
        "print(\"=\"*55)\n",
        "for layer, acc in enumerate(cap_layer_accuracies):\n",
        "    print(f\"Layer {layer:2d}: {acc:.4f}\")\n",
        "print(f\"\\nBest layer: Layer {np.argmax(cap_layer_accuracies)} with accuracy {max(cap_layer_accuracies):.4f}\")\n",
        "\n",
        "# --- Step 5: Optional visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(X.shape[1]), cap_layer_accuracies, marker='o', color='orange')\n",
        "plt.xlabel(\"BERT Layer\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Probing: Capitalization Prediction by BERT Layer\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lpFfCPt_irA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train linear models to predict the NER labels using embeddings from each layer of BERT.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "import numpy as np\n",
        "\n",
        "# Store accuracies for each layer\n",
        "layer_accuracies = []\n",
        "\n",
        "# Loop through each of the 13 layers\n",
        "for layer in range(13):\n",
        "    # Extract embeddings from the current layer\n",
        "    X_layer = X[:, layer, :]  # Shape: (num_words, 768)\n",
        "\n",
        "    # Initialize logistic regression model\n",
        "    # max_iter increased to ensure convergence\n",
        "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    # Perform 10-fold cross-validation\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(lr_model, X_layer, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "    # Calculate mean accuracy\n",
        "    mean_accuracy = scores.mean()\n",
        "    layer_accuracies.append(mean_accuracy)\n",
        "\n",
        "    # Print results for this layer\n",
        "    print(f\"Layer {layer}: Accuracy = {mean_accuracy:.4f} (std = {scores.std():.4f})\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Summary of accuracies across layers:\")\n",
        "print(\"=\"*50)\n",
        "for layer, acc in enumerate(layer_accuracies):\n",
        "    print(f\"Layer {layer:2d}: {acc:.4f}\")\n",
        "print(f\"\\nBest layer: Layer {np.argmax(layer_accuracies)} with accuracy {max(layer_accuracies):.4f}\")"
      ],
      "metadata": {
        "id": "RN0VpezfHDLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute and print the baseline accuracy of always predicting O.\n",
        "\n",
        "# Count how many times each label appears\n",
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(y)\n",
        "print(\"Label distribution:\")\n",
        "for label_id in sorted(label_counts.keys()):\n",
        "    label_name = id2label[label_id]\n",
        "    count = label_counts[label_id]\n",
        "    percentage = (count / len(y)) * 100\n",
        "    print(f\"  {label_name} (id={label_id}): {count} ({percentage:.2f}%)\")\n",
        "\n",
        "# Compute baseline accuracy (always predicting O)\n",
        "# O has label id = 0\n",
        "num_O_tags = label_counts[0]\n",
        "total_tags = len(y)\n",
        "baseline_accuracy = num_O_tags / total_tags\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Baseline Accuracy (always predicting 'O'): {baseline_accuracy:.4f}\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Compare with best layer performance\n",
        "if 'layer_accuracies' in locals():\n",
        "    best_layer_acc = max(layer_accuracies)\n",
        "    best_layer = np.argmax(layer_accuracies)\n",
        "    improvement = best_layer_acc - baseline_accuracy\n",
        "    print(f\"\\nBest layer (Layer {best_layer}): {best_layer_acc:.4f}\")\n",
        "    print(f\"Improvement over baseline: {improvement:.4f} ({improvement/baseline_accuracy*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "jZMHnMcpHGhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train linear models to predict capitalization.\n",
        "# Compute and print the accuracy of these models for each layer of BERT.\n",
        "# Compute and print the baseline accuracy.\n",
        "\n",
        "# First, create capitalization labels for each word in the dataset\n",
        "capitalization_labels = []\n",
        "\n",
        "for example in tokenized_sample:\n",
        "    tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n",
        "    labels = example['labels']\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        if label != -100:  # Only process tokens that correspond to first subword of a word\n",
        "            token = tokens[i]\n",
        "            # Check if first character is uppercase\n",
        "            # Exclude special tokens like [CLS], [SEP]\n",
        "            if token.startswith('[') and token.endswith(']'):\n",
        "                continue  # Skip special tokens\n",
        "            is_capitalized = 1 if token[0].isupper() else 0\n",
        "            capitalization_labels.append(is_capitalized)\n",
        "\n",
        "y_cap = np.array(capitalization_labels)\n",
        "\n",
        "print(f\"Total words for capitalization task: {len(y_cap)}\")\n",
        "print(f\"Number of capitalized words: {y_cap.sum()}\")\n",
        "print(f\"Number of non-capitalized words: {len(y_cap) - y_cap.sum()}\")\n",
        "\n",
        "# Compute baseline accuracy (always predicting not capitalized, i.e., 0)\n",
        "num_not_capitalized = len(y_cap) - y_cap.sum()\n",
        "baseline_cap_accuracy = num_not_capitalized / len(y_cap)\n",
        "print(f\"\\nBaseline Accuracy (always predicting not capitalized): {baseline_cap_accuracy:.4f}\")\n",
        "\n",
        "# Train logistic regression for each layer to predict capitalization\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Capitalization Prediction Accuracy by Layer\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cap_layer_accuracies = []\n",
        "\n",
        "for layer in range(13):\n",
        "    # Extract embeddings from the current layer\n",
        "    X_layer = X[:, layer, :]\n",
        "\n",
        "    # Initialize logistic regression model\n",
        "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    # Perform 10-fold cross-validation\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(lr_model, X_layer, y_cap, cv=kfold, scoring='accuracy')\n",
        "\n",
        "    # Calculate mean accuracy\n",
        "    mean_accuracy = scores.mean()\n",
        "    cap_layer_accuracies.append(mean_accuracy)\n",
        "\n",
        "    # Print results for this layer\n",
        "    print(f\"Layer {layer}: Accuracy = {mean_accuracy:.4f} (std = {scores.std():.4f})\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Summary: Capitalization vs NER\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Baseline (always not capitalized): {baseline_cap_accuracy:.4f}\")\n",
        "print(f\"Best capitalization layer: Layer {np.argmax(cap_layer_accuracies)} with accuracy {max(cap_layer_accuracies):.4f}\")\n",
        "if 'layer_accuracies' in locals():\n",
        "    print(f\"\\nBaseline (always O for NER): {baseline_accuracy:.4f}\")\n",
        "    print(f\"Best NER layer: Layer {np.argmax(layer_accuracies)} with accuracy {max(layer_accuracies):.4f}\")\n",
        "\n",
        "# Optional: Plot comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Observations:\")\n",
        "print(\"=\"*50)\n",
        "print(\"Compare the accuracies for capitalization vs NER across layers.\")\n",
        "print(\"Note which layers perform best for each task and how much they\")\n",
        "print(\"improve over their respective baselines.\")"
      ],
      "metadata": {
        "id": "EsLTZG6bHJ42"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}